<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 600px;
                 background-color: #0e1117;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/government-of-mali-launches-data-center-in-bamako/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/government-of-mali-launches-data-center-in-bamako/", "physics": true, "shape": "dot", "size": 6, "title": "Government of Mali launches data center in Bamako. \u003cp\u003eData center designed to meet Tier III standards\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#f4b000", "font": {"color": "white"}, "id": "cluster_4b8adc62-b9cf-46fb-8db8-4e87a5825164", "label": "Government / Mali / Bamako", "physics": true, "shape": "dot", "size": 61, "title": "Government / Mali / Bamako (5 signals)"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/cybastion-to-build-20mw-data-center-in-gabon/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/cybastion-to-build-20mw-data-center-in-gabon/", "physics": true, "shape": "dot", "size": 6, "title": "Cybastion to build 20MW data center in Gabon. \u003cp\u003eWorking with Gabonese government and Cisco on project\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/plan-for-213mw-data-center-blocked-by-councilors-in-edinburgh-scotland/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/plan-for-213mw-data-center-blocked-by-councilors-in-edinburgh-scotland/", "physics": true, "shape": "dot", "size": 6, "title": "Plan for 213MW data center blocked by councilors in Edinburgh, Scotland. \u003cp\u003eScheme had been recommended for approval\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/local-authorities-in-ashville-ohio-deny-annexation-request-for-data-center-project/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/local-authorities-in-ashville-ohio-deny-annexation-request-for-data-center-project/", "physics": true, "shape": "dot", "size": 6, "title": "Local authorities in Ashville, Ohio, deny annexation request for data center project. \u003cp\u003eUnclear how this will impact EdgeConneX\u0027s plans\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/ta-realty-sells-two-data-center-buildings-in-leesburg-virginia/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/ta-realty-sells-two-data-center-buildings-in-leesburg-virginia/", "physics": true, "shape": "dot", "size": 6, "title": "TA Realty sells two data center buildings in Leesburg, Virginia. \u003cp\u003eFirst of five fully leased buildings at campus\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/denmarks-thylander-secures-outside-investors-for-data-center-venture/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/denmarks-thylander-secures-outside-investors-for-data-center-venture/", "physics": true, "shape": "dot", "size": 6, "title": "Denmark\u2019s Thylander secures outside investors for data center venture. \u003cp\u003eCompany working on 100MW projects in Esbjerg\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#f4b000", "font": {"color": "white"}, "id": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "label": "The / Software / Announce", "physics": true, "shape": "dot", "size": 80, "title": "The / Software / Announce (20 signals)"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/aws-has-never-retired-an-nvidia-a100-server-ceo-matt-garman-claims/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/aws-has-never-retired-an-nvidia-a100-server-ceo-matt-garman-claims/", "physics": true, "shape": "dot", "size": 6, "title": "AWS has \u201cnever retired\u201d an Nvidia A100 server, CEO Matt Garman claims. \u003cp\u003eDemand still high from those running HPC workloads\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/nvidia-prologis-epri-infrapartners-target-prefab-data-centers-at-substation-sites/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/nvidia-prologis-epri-infrapartners-target-prefab-data-centers-at-substation-sites/", "physics": true, "shape": "dot", "size": 6, "title": "Nvidia, Prologis, EPRI, InfraPartners target prefab data centers at substation sites. \u003cp\u003eFive pilot projects planned in US in 2026\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/oracle-nabs-881m-cloud-contract-from-us-air-force/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/oracle-nabs-881m-cloud-contract-from-us-air-force/", "physics": true, "shape": "dot", "size": 6, "title": "Oracle nabs $88.1m cloud contract from US Air Force. \u003cp\u003eThe latest contract issued under the Cloud One program\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/ai-compute-company-subgen-ai-signs-cloud-agreement-with-alibaba-cloud-netherlands/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/ai-compute-company-subgen-ai-signs-cloud-agreement-with-alibaba-cloud-netherlands/", "physics": true, "shape": "dot", "size": 6, "title": "AI compute company Subgen AI signs cloud agreement with Alibaba Cloud Netherlands. \u003cp\u003eWill give Subgen AI clients access to Alibaba\u0027s infrastructure and technologies\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/californian-senator-proposes-bill-to-create-new-rate-class-for-large-load-data-centers/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/californian-senator-proposes-bill-to-create-new-rate-class-for-large-load-data-centers/", "physics": true, "shape": "dot", "size": 6, "title": "Californian Senator proposes bill to create new rate class for large load data centers. \u003cp\u003eWould apply to facilities with a capacity of 75MW or more\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/tennessee-utility-tva-projects-data-center-load-to-double-across-service-area-by-2030/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/tennessee-utility-tva-projects-data-center-load-to-double-across-service-area-by-2030/", "physics": true, "shape": "dot", "size": 6, "title": "Tennessee utility TVA projects data center load to double across service area by 2030. \u003cp\u003eUtility aims to construct 6.2GW of new generation assets to meet increased demand\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/multi-gigawatt-data-center-campus-planned-in-east-texas/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/multi-gigawatt-data-center-campus-planned-in-east-texas/", "physics": true, "shape": "dot", "size": 6, "title": "Multi-gigawatt data center campus planned in East Texas. \u003cp\u003eFormer paper mill site in Lufkin targeted by Amp Energy\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/tsmc-to-produce-3nm-ai-chips-at-second-fab-in-kumamoto-japan/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/tsmc-to-produce-3nm-ai-chips-at-second-fab-in-kumamoto-japan/", "physics": true, "shape": "dot", "size": 6, "title": "TSMC to produce 3nm AI chips at second fab in Kumamoto, Japan. \u003cp\u003eReverses previous decision from the company to manufacture 7nm chips at the site\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/how-ai-labs-are-solving-the-power", "label": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/how-ai-labs-are-solving-the-power", "physics": true, "shape": "dot", "size": 6, "title": "How AI Labs Are Solving the Power Crisis: The Onsite Gas Deep Dive. Bring Your Own Generation, Sayonara Electric Grid, Turbines vs. Recips. vs. Fuel Cells, Why Not Build More CCGTs?, Onsite Power TCO"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/microsofts-ai-strategy-deconstructed", "label": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/microsofts-ai-strategy-deconstructed", "physics": true, "shape": "dot", "size": 6, "title": "Microsoft\u0027s AI Strategy Deconstructed - From Energy to Tokens. \"The Big Pause\", AI Tokens Factory Economics Stack, OpenAI, Neocloud Renting, GitHub Copilot Woes, MAI and Maia Floundering"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/aws-trainium3-deep-dive-a-potential", "label": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/aws-trainium3-deep-dive-a-potential", "physics": true, "shape": "dot", "size": 6, "title": "AWS Trainium3 Deep Dive | A Potential Challenger Approaching. NL72x2/NL32x2 Scale Up Rack Architecture, Step-Function Software \u0026 System Improvements, Optimized Perf per TCO, \u0026#8220;Amazon Basics\u0026#8221; GB200 NVL36x2, Astera Labs, Trainium4"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/amazons-ai-resurgence-aws-anthropics-multi-gigawatt-trainium-expansion", "label": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/amazons-ai-resurgence-aws-anthropics-multi-gigawatt-trainium-expansion", "physics": true, "shape": "dot", "size": 6, "title": "Amazon\u2019s AI Resurgence: AWS \u0026 Anthropic\u0027s Multi-Gigawatt Trainium Expansion. Anthropic multi-gigawatt clusters, Trainium ramp, best TCO per memory bandwidth, system-level roadmap, Bedrock and internal models"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/tpuv7-google-takes-a-swing-at-the", "label": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/tpuv7-google-takes-a-swing-at-the", "physics": true, "shape": "dot", "size": 6, "title": "TPUv7: Google Takes a Swing at the King. Anthropic\u0026#8217;s 1GW+ TPUs, New customers Meta/SSI/xAI/OAI, Full Stack Review of v7 Ironwood, CUDA Moat at risk, Next Generation TPUv8AX and TPUv8X versus Vera Rubin"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/xais-colossus-2-first-gigawatt-datacenter", "label": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/xais-colossus-2-first-gigawatt-datacenter", "physics": true, "shape": "dot", "size": 6, "title": "xAI\u0027s Colossus 2 - First Gigawatt Datacenter In The World, Unique RL Methodology, Capital Raise. On Site Turbines, Mississippi Expansion, Solaris Energy, Can xAI afford it?, Middle East Funding, Tesla, Talent Exodus, API revenue, Consumer Growth, RL Environment"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/another-giant-leap-the-rubin-cpx-specialized-accelerator-rack", "label": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/another-giant-leap-the-rubin-cpx-specialized-accelerator-rack", "physics": true, "shape": "dot", "size": 6, "title": "Another Giant Leap: The Rubin CPX Specialized Accelerator \u0026 Rack. New Prefill Specialized GPU, Rack Architecture, BOM, Disaggregated PD, Higher Perf per TCO, Lower TCO, GDDR7 \u0026 HBM Market Trends"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/huawei-ascend-production-ramp", "label": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/huawei-ascend-production-ramp", "physics": true, "shape": "dot", "size": 6, "title": "Huawei Ascend Production Ramp: Die Banks, TSMC Continued Production, HBM is The Bottleneck. H20 Shipments, Blackwell B30A, Bottlenecks to Chinese Chip Production, Export Controls, CXMT, SMIC, Cambricon"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/h100-vs-gb200-nvl72-training-benchmarks", "label": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/h100-vs-gb200-nvl72-training-benchmarks", "physics": true, "shape": "dot", "size": 6, "title": "H100 vs GB200 NVL72 Training Benchmarks - Power, TCO, and Reliability Analysis, Software Improvement Over Time. Joules per Token, TCO Per Million Tokens, MFU, Tokens Per US Annual Household Energy Usage, DeepSeek 670B, GB200 Unreliability, Backplane Downtime"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03969v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03969v1", "physics": true, "shape": "dot", "size": 6, "title": "Structural shifts in institutional participation and collaboration within the AI arXiv preprint research ecosystem. arXiv:2602.03969v1 Announce Type: cross \nAbstract: The emergence of large language models (LLMs) represents a significant technological shift within the scientific ecosystem, particularly within the field of artificial intelligence (AI). This paper examines structural changes in the AI research landscape using a dataset of arXiv preprints (cs.AI) from 2021 through 2025. Given the rapid pace of AI development, the preprint ecosystem has become a critical barometer for real-time scientific shifts, often preceding formal peer-reviewed publication by months or years. By employing a multi-stage data collection and enrichment pipeline in conjunction with LLM-based institution classification, we analyze the evolution of publication volumes, author team sizes, and academic--industry collaboration patterns. Our results reveal an unprecedented surge in publication output following the introduction of ChatGPT, with academic institutions continuing to provide the largest volume of research. However, we observe that academic--industry collaboration is still suppressed, as measured by a Normalized Collaboration Index (NCI) that remains significantly below the random-mixing baseline across all major subfields. These findings highlight a continuing institutional divide and suggest that the capital-intensive nature of generative AI research may be reshaping the boundaries of scientific collaboration."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2510.26275v3", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2510.26275v3", "physics": true, "shape": "dot", "size": 6, "title": "A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI. arXiv:2510.26275v3 Announce Type: replace-cross \nAbstract: Generative AI (GenAI) is rapidly transforming software engineering (SE) practices, influencing how SE processes are executed, as well as how software systems are developed, operated, and evolved. This paper applies design science research to build a roadmap for GenAI-augmented SE. The process consists of three cycles that incrementally integrate multiple sources of evidence, including collaborative discussions from the FSE 2025 \"Software Engineering 2030\" workshop, rapid literature reviews, and external feedback sessions involving peers. McLuhan\u0027s tetrads were used as a conceptual instrument to systematically capture the transforming effects of GenAI on SE processes and software products. The resulting roadmap identifies four fundamental forms of GenAI augmentation in SE and systematically characterizes their related research challenges and opportunities. These insights are then consolidated into a set of future research directions. By grounding the roadmap in a rigorous multi-cycle process and cross-validating it among independent author teams and peers, the study provides a transparent and reproducible foundation for analyzing how GenAI affects SE processes, methods and tools, and for framing future research within this rapidly evolving area."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/inferencemax-open-source-inference", "label": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/inferencemax-open-source-inference", "physics": true, "shape": "dot", "size": 6, "title": "InferenceMAX\u2122: Open Source Inference Benchmarking. NVIDIA GB200 NVL72, AMD MI355X, Throughput Token per GPU, Latency Tok/s/user, Perf per Dollar, Tokens per Provisioned Megawatt, DeepSeek R1 670B, GPTOSS 120B, Llama3 70B"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#f4b000", "font": {"color": "white"}, "id": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "label": "We / Announce / Type", "physics": true, "shape": "dot", "size": 120, "title": "We / Announce / Type (290 signals)"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03900v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03900v1", "physics": true, "shape": "dot", "size": 6, "title": "Knowledge Model Prompting Increases LLM Performance on Planning Tasks. arXiv:2602.03900v1 Announce Type: new \nAbstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs\u0027 ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework\u0027s unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03950v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03950v1", "physics": true, "shape": "dot", "size": 6, "title": "Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation. arXiv:2602.03950v1 Announce Type: new \nAbstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03955v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03955v1", "physics": true, "shape": "dot", "size": 6, "title": "AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent. arXiv:2602.03955v1 Announce Type: new \nAbstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04496v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04496v1", "physics": true, "shape": "dot", "size": 6, "title": "ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control. arXiv:2602.04496v1 Announce Type: new \nAbstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity\u0027s Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03975v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03975v1", "physics": true, "shape": "dot", "size": 6, "title": "Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure. arXiv:2602.03975v1 Announce Type: new \nAbstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \\emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \\textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\\% fewer verifier calls."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03978v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03978v1", "physics": true, "shape": "dot", "size": 6, "title": "Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning. arXiv:2602.03978v1 Announce Type: new \nAbstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a \"free gift\" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03994v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03994v1", "physics": true, "shape": "dot", "size": 6, "title": "When Chains of Thought Don\u0027t Matter: Causal Bypass in Large Language Models. arXiv:2602.03994v1 Announce Type: cross \nAbstract: Chain-of-thought (CoT) prompting is widely assumed to expose a model\u0027s reasoning process and improve transparency. We attempted to enforce this assumption by penalizing unfaithful reasoning, but found that surface-level compliance does not guarantee causal reliance. Our central finding is negative: even when CoT is verbose, strategic, and flagged by surface-level manipulation detectors, model answers are often causally independent of the CoT content. We present a diagnostic framework for auditing this failure mode: it combines (i) an interpretable behavioral module that scores manipulation-relevant signals in CoT text and (ii) a causal probe that measures CoT-mediated influence (CMI) via hidden-state patching and reports a bypass score ($1-\\mathrm{CMI}$), quantifying the degree to which the answer is produced by a bypass circuit independent of the rationale. In pilot evaluations, audit-aware prompting increases detectable manipulation signals (mean risk-score delta: $+5.10$), yet causal probes reveal task-dependent mediation: many QA items exhibit near-total bypass (CMI $\\approx 0$), while some logic problems show stronger mediation (CMI up to $0.56$). Layer-wise analysis reveals narrow and task-dependent ``reasoning windows\u0027\u0027 even when mean CMI is low."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04089v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04089v1", "physics": true, "shape": "dot", "size": 6, "title": "Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL. arXiv:2602.04089v1 Announce Type: new \nAbstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04101v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04101v1", "physics": true, "shape": "dot", "size": 6, "title": "Interfaze: The Future of AI is built on Task-Specific Small Models. arXiv:2602.04101v1 Announce Type: new \nAbstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.\n  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04144v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04144v1", "physics": true, "shape": "dot", "size": 6, "title": "OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows. arXiv:2602.04144v1 Announce Type: new \nAbstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \\textbf{\\underline{O}}mni-\\textbf{\\underline{M}}odality \\textbf{\\underline{G}}eneration Agent (\\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \\textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\\% missing rates."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04210v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04210v1", "physics": true, "shape": "dot", "size": 6, "title": "Steering LLMs via Scalable Interactive Oversight. arXiv:2602.04210v1 Announce Type: new \nAbstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \\emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04213v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04213v1", "physics": true, "shape": "dot", "size": 6, "title": "InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons. arXiv:2602.04213v1 Announce Type: new \nAbstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent\u0027s performance, and review the agent\u0027s decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04248v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04248v1", "physics": true, "shape": "dot", "size": 6, "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search. arXiv:2602.04248v1 Announce Type: new \nAbstract: Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04284v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04284v1", "physics": true, "shape": "dot", "size": 6, "title": "Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning. arXiv:2602.04284v1 Announce Type: new \nAbstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent\u0027s adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04326v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04326v1", "physics": true, "shape": "dot", "size": 6, "title": "From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents. arXiv:2602.04326v1 Announce Type: new \nAbstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators\u0027 intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04572v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04572v1", "physics": true, "shape": "dot", "size": 6, "title": "From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums. arXiv:2602.04572v1 Announce Type: new \nAbstract: While Generative AI (GenAI) systems draw users away from (Q\u0026amp;A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04575v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04575v1", "physics": true, "shape": "dot", "size": 6, "title": "Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration. arXiv:2602.04575v1 Announce Type: new \nAbstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling\u0027\u0027 manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator\u0027s high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \\textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.\n  Under this paradigm, the user\u0027s role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe\u0027\u0027 into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04634v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04634v1", "physics": true, "shape": "dot", "size": 6, "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning. arXiv:2602.04634v1 Announce Type: new \nAbstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04813v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04813v1", "physics": true, "shape": "dot", "size": 6, "title": "Agentic AI in Healthcare \u0026 Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents. arXiv:2602.04813v1 Announce Type: new \nAbstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation \u0026 Learning, Safety \u0026 Ethics, Framework Typology and Core Tasks \u0026 Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection \u0026 Mitigation sub-dimension under Adaptation \u0026 Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks \u0026 Subtasks, information centric capabilities lead e.g., Medical Question Answering \u0026 Decision Support and Benchmarking \u0026 Simulation, while action and discovery oriented areas such as Treatment Planning \u0026 Prescription still show substantial gaps (~59% Not Implemented)."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04837v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04837v1", "physics": true, "shape": "dot", "size": 6, "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing. arXiv:2602.04837v1 Announce Type: new \nAbstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04843v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04843v1", "physics": true, "shape": "dot", "size": 6, "title": "Fluid Representations in Reasoning Models. arXiv:2602.04843v1 Announce Type: new \nAbstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03849v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03849v1", "physics": true, "shape": "dot", "size": 6, "title": "HybridQuestion: Human-AI Collaboration for Identifying High-Impact Research Questions. arXiv:2602.03849v1 Announce Type: cross \nAbstract: The \"AI Scientist\" paradigm is transforming scientific research by automating key stages of the research process, from idea generation to scholarly writing. This shift is expected to accelerate discovery and expand the scope of scientific inquiry. However, a key question remains unclear: can AI scientists identify meaningful research questions? While Large Language Models (LLMs) have been applied successfully to task-specific ideation, their potential to conduct strategic, long-term assessments of past breakthroughs and future questions remains largely unexplored. To address this gap, we explore a human-AI hybrid solution that integrates the scalable data processing capabilities of AI with the value judgment of human experts. Our methodology is structured in three phases. The first phase, AI-Accelerated Information Gathering, leverages AI\u0027s advantage in processing vast amounts of literature to generate a hybrid information base. The second phase, Candidate Question Proposing, utilizes this synthesized data to prompt an ensemble of six diverse LLMs to propose an initial candidate pool, filtered via a cross-model voting mechanism. The third phase, Hybrid Question Selection, refines this pool through a multi-stage filtering process that progressively increases human oversight. To validate this system, we conducted an experiment aiming to identify the Top 10 Scientific Breakthroughs of 2025 and the Top 10 Scientific Questions for 2026 across five major disciplines. Our analysis reveals that while AI agents demonstrate high alignment with human experts in recognizing established breakthroughs, they exhibit greater divergence in forecasting prospective questions, suggesting that human judgment remains crucial for evaluating subjective, forward-looking challenges."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03876v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03876v1", "physics": true, "shape": "dot", "size": 6, "title": "GOPO: Policy Optimization using Ranked Rewards. arXiv:2602.03876v1 Announce Type: cross \nAbstract: Standard reinforcement learning from human feedback (RLHF) trains a reward model on pairwise preference data and then uses it for policy optimization. However, while reward models are optimized to capture relative preferences, existing policy optimization techniques rely on absolute reward magnitudes during training. In settings where the rewards are non-verifiable such as summarization, instruction following, and chat completion, this misalignment often leads to suboptimal performance. We introduce Group Ordinal Policy Optimization (GOPO), a policy optimization method that uses only the ranking of the rewards and discards their magnitudes. Our rank-based transformation of rewards provides several gains, compared to Group Relative Policy Optimization (GRPO), in settings with non-verifiable rewards: (1) consistently higher training/validation reward trajectories, (2) improved LLM-as-judge evaluations across most intermediate training steps, and (3) reaching a policy of comparable quality in substantially less training steps than GRPO. We demonstrate consistent improvements across a range of tasks and model sizes."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03949v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03949v1", "physics": true, "shape": "dot", "size": 6, "title": "Semantic Rate Distortion and Posterior Design: Compute Constraints, Multimodality, and Strategic Inference. arXiv:2602.03949v1 Announce Type: cross \nAbstract: We study strategic Gaussian semantic compression under rate and compute constraints, where an encoder and decoder optimize distinct quadratic objectives. A latent Gaussian state generates a task dependent semantic variable, and the decoder best responds via MMSE estimation, reducing the encoder\u0027s problem to posterior covariance design under an information rate constraint. We characterize the strategic rate distortion function in direct, remote, and full information regimes, derive semantic waterfilling and rate constrained Gaussian persuasion solutions, and establish Gaussian optimality under misaligned objectives. We further show that architectural compute limits act as implicit rate constraints, yielding exponential improvements in semantic accuracy with model depth and inference time compute, while multimodal observation eliminates the geometric mean penalty inherent to remote encoding. These results provide information theoretic foundations for data and energy efficient AI and offer a principled interpretation of modern multimodal language models as posterior design mechanisms under resource constraints."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#ff9500", "font": {"color": "white"}, "id": "collapsed_b4e77b73-9c9a-434d-9565-fa20898eb175", "label": "+265 more", "physics": true, "shape": "box", "size": 12, "title": "This cluster has 265 more signals not shown in the graph.\nSelect the cluster below to view all 290 signals."}]);
                  edges = new vis.DataSet([{"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/government-of-mali-launches-data-center-in-bamako/", "smooth": false, "to": "cluster_4b8adc62-b9cf-46fb-8db8-4e87a5825164", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/cybastion-to-build-20mw-data-center-in-gabon/", "smooth": false, "to": "cluster_4b8adc62-b9cf-46fb-8db8-4e87a5825164", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/plan-for-213mw-data-center-blocked-by-councilors-in-edinburgh-scotland/", "smooth": false, "to": "cluster_4b8adc62-b9cf-46fb-8db8-4e87a5825164", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/local-authorities-in-ashville-ohio-deny-annexation-request-for-data-center-project/", "smooth": false, "to": "cluster_4b8adc62-b9cf-46fb-8db8-4e87a5825164", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/ta-realty-sells-two-data-center-buildings-in-leesburg-virginia/", "smooth": false, "to": "cluster_4b8adc62-b9cf-46fb-8db8-4e87a5825164", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/denmarks-thylander-secures-outside-investors-for-data-center-venture/", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/aws-has-never-retired-an-nvidia-a100-server-ceo-matt-garman-claims/", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/nvidia-prologis-epri-infrapartners-target-prefab-data-centers-at-substation-sites/", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/oracle-nabs-881m-cloud-contract-from-us-air-force/", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/ai-compute-company-subgen-ai-signs-cloud-agreement-with-alibaba-cloud-netherlands/", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/californian-senator-proposes-bill-to-create-new-rate-class-for-large-load-data-centers/", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/tennessee-utility-tva-projects-data-center-load-to-double-across-service-area-by-2030/", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/multi-gigawatt-data-center-campus-planned-in-east-texas/", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/tsmc-to-produce-3nm-ai-chips-at-second-fab-in-kumamoto-japan/", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/how-ai-labs-are-solving-the-power", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/microsofts-ai-strategy-deconstructed", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/aws-trainium3-deep-dive-a-potential", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/amazons-ai-resurgence-aws-anthropics-multi-gigawatt-trainium-expansion", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/tpuv7-google-takes-a-swing-at-the", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/xais-colossus-2-first-gigawatt-datacenter", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/another-giant-leap-the-rubin-cpx-specialized-accelerator-rack", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/huawei-ascend-production-ramp", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/h100-vs-gb200-nvl72-training-benchmarks", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03969v1", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2510.26275v3", "smooth": false, "to": "cluster_a5098656-8447-45f2-bcfa-c1e1a29cd661", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://semianalysis.substack.com/feed::https://newsletter.semianalysis.com/p/inferencemax-open-source-inference", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03900v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(14, 17, 23, 0.3)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03950v1", "smooth": false, "to": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03955v1", "value": 0.6837915184896336, "width": 1}, {"color": "rgba(14, 17, 23, 0.3)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03950v1", "smooth": false, "to": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04496v1", "value": 0.677362455397959, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03950v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(14, 17, 23, 0.3)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03955v1", "smooth": false, "to": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04496v1", "value": 0.6968424903603008, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03955v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03975v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(14, 17, 23, 0.3)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03978v1", "smooth": false, "to": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03994v1", "value": 0.6577072113223036, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03978v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04089v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04101v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04144v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04210v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04213v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04248v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04284v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04326v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04496v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04572v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04575v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04634v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04813v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04837v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.04843v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03849v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03876v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03949v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2602.03994v1", "smooth": false, "to": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}, {"color": "#ff9500", "dashes": true, "from": "cluster_b4e77b73-9c9a-434d-9565-fa20898eb175", "smooth": false, "to": "collapsed_b4e77b73-9c9a-434d-9565-fa20898eb175", "value": 0.9, "width": 1}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"nodes": {"borderWidth": 0, "borderWidthSelected": 0, "chosen": {"node": false, "label": false}, "font": {"strokeWidth": 0}}, "interaction": {"hover": true, "hoverConnectedEdges": false, "selectConnectedEdges": false, "dragNodes": true, "dragView": true, "zoomView": true}, "physics": {"enabled": true, "stabilization": {"enabled": true, "iterations": 200, "updateInterval": 25, "fit": true}, "barnesHut": {"gravitationalConstant": -8000, "centralGravity": 0.3, "springLength": 150, "springConstant": 0.04, "damping": 0.09, "avoidOverlap": 0.5}, "solver": "barnesHut"}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    
    <script type="text/javascript">
        var selectedNode = null;
        var physicsDisabled = false;
        
        // Disable physics after stabilization so nodes stay where moved
        network.once("stabilizationIterationsDone", function() {
            network.setOptions({ physics: { enabled: false } });
            physicsDisabled = true;
        });
        
        // When dragging a cluster node, move all connected signal nodes with it
        var dragStartPositions = {};
        var clusterStartPosition = {};
        var isDraggingCluster = false;
        
        network.on("dragStart", function(params) {
            if (params.nodes.length > 0) {
                var nodeId = params.nodes[0];
                
                // Check if this is a cluster node
                if (nodeId.startsWith('cluster_')) {
                    isDraggingCluster = true;
                    // Store initial position of cluster
                    clusterStartPosition = network.getPositions([nodeId])[nodeId];
                    
                    // Get all connected signal nodes
                    var connectedNodes = network.getConnectedNodes(nodeId);
                    dragStartPositions = {};
                    
                    connectedNodes.forEach(function(connectedId) {
                        if (!connectedId.startsWith('cluster_')) {
                            dragStartPositions[connectedId] = network.getPositions([connectedId])[connectedId];
                        }
                    });
                } else {
                    // Dragging a signal node - allow independent movement
                    isDraggingCluster = false;
                }
            }
        });
        
        network.on("dragging", function(params) {
            if (params.nodes.length > 0) {
                var nodeId = params.nodes[0];
                
                // If dragging a cluster, move connected signals
                if (isDraggingCluster && nodeId.startsWith('cluster_') && Object.keys(dragStartPositions).length > 0) {
                    var currentPos = network.getPositions([nodeId])[nodeId];
                    var dx = currentPos.x - clusterStartPosition.x;
                    var dy = currentPos.y - clusterStartPosition.y;
                    
                    // Move all connected signal nodes by the same offset
                    for (var connectedId in dragStartPositions) {
                        var oldPos = dragStartPositions[connectedId];
                        var newX = oldPos.x + dx;
                        var newY = oldPos.y + dy;
                        network.moveNode(connectedId, newX, newY);
                    }
                }
                // If dragging a signal node, it moves independently (default behavior)
            }
        });
        
        network.on("dragEnd", function(params) {
            // Clear drag state
            dragStartPositions = {};
            clusterStartPosition = {};
            isDraggingCluster = false;
        });
        
        network.on("click", function(params) {
            if (params.nodes.length > 0) {
                var nodeId = params.nodes[0];
                var node = network.body.data.nodes.get(nodeId);
                
                // Create persistent popup
                var popup = document.createElement('div');
                popup.id = 'node-popup';
                popup.style.position = 'fixed';
                popup.style.background = '#1e1e1e';
                popup.style.color = 'white';
                popup.style.padding = '15px';
                popup.style.borderRadius = '8px';
                popup.style.maxWidth = '400px';
                popup.style.border = '2px solid #f4b000';
                popup.style.zIndex = '10000';
                popup.style.top = '50%';
                popup.style.left = '50%';
                popup.style.transform = 'translate(-50%, -50%)';
                popup.style.boxShadow = '0 4px 20px rgba(0,0,0,0.5)';
                popup.innerHTML = '<strong>' + (node.label || 'Signal') + '</strong><br><br>' + 
                                 (node.title || 'No content') + 
                                 '<br><br><small style="color: #888;">Click anywhere to close</small>';
                
                // Remove existing popup if any
                var existing = document.getElementById('node-popup');
                if (existing) existing.remove();
                
                document.body.appendChild(popup);
                selectedNode = nodeId;
            } else {
                // Click on empty space - remove popup
                var popup = document.getElementById('node-popup');
                if (popup) popup.remove();
                selectedNode = null;
            }
        });
    </script>
    </body>
</html>